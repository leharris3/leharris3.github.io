<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Building AI With Eyes | Levi Harris
    
  
</title>
<meta name="author" content="Levi Harris">
<meta name="description" content="">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://0.0.0.0:8080/blog/2024/building-ai-with-eyes/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class=" ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Levi</span>
            
            
            Harris
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/publications/">publications
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/repositories/">repositories
                    
                  </a>
                </li>
              
            
          
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        







<div class="post">
  <header class="post-header">
    <h1 class="post-title">Building AI With Eyes</h1>
    <p class="post-meta">
      Created in March 18, 2024
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>
      
      

      
    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>On a long car ride home from Chapel Hill, I listened to a fantastic <a href="https://www.youtube.com/watch?v=5t1vTLU7s40&amp;ab_channel=LexFridman" rel="external nofollow noopener" target="_blank">interview</a> between Yan Lecunn and Lex Fridman. The two discussed, among many other things, the fundamental limitations of Large Language Models (LLMs). Yan mentioned that for a an entity to exhibit intelligence it must display at a minimum :</p>

<ol>
  <li>Persistent memory.</li>
  <li>A capacity to understand the physical world.</li>
  <li>The ability to reason.</li>
  <li>The ability to plan.</li>
</ol>

<p>LLMs, in their current state, display none of these qualities. I believe, as does Yan, that natural language is just too crude of an approximation of the natural world to build a complete world model. LLMs need the ability to see and hear as well. This is one of the core ideas behind Computer Vision (CV): understanding the world through sight. And so, this brings us to a primary goal of AI. We seek to build systems that can solve complicated tasks in the real world across a variety of domains. The precursor to this challenge is developing robust world models that AI can easily generalize from.</p>

<p>LLMs represent the best world models constructed to date. SOTA LLMs can generalize to basically any task which requires use of natural language. For example, writing poems, solving coding problems, basic mathematics, and so on. These models, however, lack a robust understanding of the real world. This fact has much to do with the way these models distill a world model (i.e., their pre-training). LLMs are pre-trained on next-token prediction using a large text corpus. This paradigm is made especially convenient by the fact that words (unlike images) are discrete units of information.</p>

<p>So now, Yan asks the obvious question: how do you pre-train large models on continuous data types like images? The motivation here of course is to create a model with a general understanding of the physical world, not just language. The answer turns out to be somewhat tricky. The naive approach of course is to predict the next frame in videos. This strategy doesn’t work for a number of reasons. This roadblock has something to do with the continuous nature of video and the presence of a number of latent variables. Simply put, self-supervised methods are ineffective.</p>

<p>What about supervised methods? Let’s pre-train a huge vision model using image classification. This also turns out to be tricky. Another naive idea is to train a classifier using a bunch of images and their augmented counter parts. But this doesn’t work either. The secret sauce it turns out is <em>contrastive learning.</em> An image and an augmented version of it (e.g., a masked, distorted variation, etc.) are encoded together as a joint embedding. This vector is used the predict an object class. Do this enough times and you’ve built a very powerful, general purpose vision backbone. Why does this work? At a high level it seems important to teach a model both relevant <em>and</em> irrelevant features of an image simultaneously. In NLP during the pre-training stage LLMs learn both likely <em>and</em> unlikely continuations. We simply extent this idea to images.</p>

<p>FAIR at Meta have developed a number of techniques that leverage this approach. Mainly, <a href="https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/" rel="external nofollow noopener" target="_blank">V-JEPA</a> and <a href="https://ai.meta.com/blog/dino-v2-computer-vision-self-supervised-learning/" rel="external nofollow noopener" target="_blank">DINO</a>. It is important to note that building a unified language-vision backbone is only step 2 (<em>a capacity to understand the physical world</em>) along the path to AGI. Developing models with persistent memory and legitimate reasoning abilities remain outstanding challenges. I similarly agree with Yan that machine intelligence will scale linearly with time. The path to smarter than human, general purpose AI will span a long and challenging road. Still, I am greatly excited that the fine folks at Meta are sharing their work publicly. The ideas are simple, but the approaches that will lead us to AGI remain unclear. It looks like AI researcher have their work cut out for them.</p>

    </div>
  </article>

  

  

  
    
      

  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/sex-and-cells/">Sex &amp; Cells</a>
  </li>



    
  

  
  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2025
      Levi
      
      Harris. 
      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    



    

    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
